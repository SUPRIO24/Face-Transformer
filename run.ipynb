{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Transformer for Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " colab.ipynb\t\t\t\t requirements-base.txt\n",
      " config.py\t\t\t\t requirements.txt\n",
      " Data\t\t\t\t\t results\n",
      " eval\t\t\t\t\t run.ipynb\n",
      "'Face Transformer for Recognition.pdf'\t test_forward.py\n",
      " image_iter.py\t\t\t\t test.py\n",
      " images\t\t\t\t\t train.py\n",
      " LICENSE\t\t\t\t util\n",
      " __pycache__\t\t\t\t vit_pytorch\n",
      " README.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Line Interpretation\n",
    "\n",
    "- **`CUDA_VISIBLE_DEVICES`** : Enter the GPU ID (`0`, `1`, `2`, `3`).\n",
    "\n",
    "- **`-w` (`--workers_id`)** : Enter the Worker ID (`0`, `1`, `2`, `3`) [Same as GPU ID].\n",
    "\n",
    "- **`-b` (`--batch_size`)** : Batch Size [Here it refers to `13` for `retina` & `16` for `casia` (for **Nvidia GeForce GTX 1650 4GB GPU**). Change according to GPU memory].\n",
    "\n",
    "- **`-d` (`--data_mode`)** : Use which Database [`casia`, `vgg`, `ms1m`, `retina`, `ms1mr`]. Here it refers to `retina` & `casia`.\n",
    "\n",
    "- **`-n` (`--net`)** : Which Network [`VIT`, `VITs`, `SWT`, `NAT`]. Here it refers to `VIT` & `VITs`.\n",
    "\n",
    "- **`--outdir` (Output Directory)** : Change the `output_dir` to the name of the dataset in the `./models/ViT-PxSx_<output_dir>_cosface_sx`. Here it refers to `retina` & `casia`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ViT-P8S8`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P8S8_retina_cosface_s1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_ID [0]\n",
      "============================================================\n",
      "Overall Configurations:\n",
      "{'SEED': 1337, 'INPUT_SIZE': [112, 112], 'EMBEDDING_SIZE': 512, 'GPU_ID': [0], 'DEVICE': device(type='cuda', index=0), 'MULTI_GPU': False, 'NUM_EPOCH': 1, 'BATCH_SIZE': 13, 'DATA_ROOT': './Data/ms1m-retinaface-t1/', 'EVAL_PATH': './eval/', 'BACKBONE_NAME': 'VIT', 'HEAD_NAME': 'CosFace', 'TARGET': ['lfw'], 'BACKBONE_RESUME_ROOT': '', 'WORK_PATH': './results/ViT-P8S8_retina_cosface_s1'}\n",
      "============================================================\n",
      "./Data/ms1m-retinaface-t1/train.rec ./Data/ms1m-retinaface-t1/train.idx\n",
      "header0 label [5179511. 5272942.]\n",
      "id2range 93431\n",
      "Number of Training Classes: 93431\n",
      "./eval/lfw.bin\n",
      "loading bin 0\n",
      "loading bin 1000\n",
      "loading bin 2000\n",
      "loading bin 3000\n",
      "loading bin 4000\n",
      "loading bin 5000\n",
      "loading bin 6000\n",
      "loading bin 7000\n",
      "loading bin 8000\n",
      "loading bin 9000\n",
      "loading bin 10000\n",
      "loading bin 11000\n",
      "torch.Size([12000, 3, 112, 112])\n",
      "ver lfw\n",
      "self.device_id [0]\n",
      "self.device_id [0]\n",
      "============================================================\n",
      "ViT_face(\n",
      "  (patch_to_embedding): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0-19): 20 x ModuleList(\n",
      "        (0): Residual(\n",
      "          (fn): PreNorm(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (1): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Residual(\n",
      "          (fn): PreNorm(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (1): GELU(approximate='none')\n",
      "                (2): Dropout(p=0.1, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (4): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_latent): Identity()\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (loss): CosFace(in_features = 512, out_features = 93431, s = 64.0, m = 0.35)\n",
      ")\n",
      "VIT Backbone Generated\n",
      "============================================================\n",
      "============================================================\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0003\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0003\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Optimizer Generated\n",
      "============================================================\n",
      "Epoch 1 Batch 10\tSpeed: 17.79 samples/s\tTraining Loss 36.6559 (37.8972)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 20\tSpeed: 33.87 samples/s\tTraining Loss 38.0961 (37.6846)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 30\tSpeed: 33.69 samples/s\tTraining Loss 38.2550 (37.4803)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 40\tSpeed: 33.69 samples/s\tTraining Loss 38.2852 (37.8466)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 50\tSpeed: 36.68 samples/s\tTraining Loss 38.1298 (37.7678)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 60\tSpeed: 47.17 samples/s\tTraining Loss 36.8421 (37.5125)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 70\tSpeed: 46.96 samples/s\tTraining Loss 38.3116 (37.6107)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 80\tSpeed: 47.19 samples/s\tTraining Loss 40.0392 (38.3536)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 90\tSpeed: 46.92 samples/s\tTraining Loss 38.7938 (37.9952)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 100\tSpeed: 46.93 samples/s\tTraining Loss 38.1754 (37.5754)\tTraining Prec@1 0.000 (0.000)\n",
      "Learning rate 0.000001\n",
      "Perform Evaluation on ['lfw'] , and Save Checkpoints...\n",
      "(12000, 512)\n",
      "[lfw][100]XNorm: 22.62751\n",
      "[lfw][100]Accuracy-Flip: 0.54750+-0.01957\n",
      "[lfw][100]Best-Threshold: 0.01000\n",
      "highest_acc: [0.5475]\n",
      "Epoch 1 Batch 110\tSpeed: 0.92 samples/s\tTraining Loss 37.0317 (37.6172)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 120\tSpeed: 46.25 samples/s\tTraining Loss 37.2291 (37.5439)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 130\tSpeed: 46.21 samples/s\tTraining Loss 37.0690 (37.7481)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 140\tSpeed: 45.96 samples/s\tTraining Loss 37.7058 (37.3964)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 150\tSpeed: 46.19 samples/s\tTraining Loss 37.9788 (37.6406)\tTraining Prec@1 0.000 (0.000)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debargha/Workspace/Projects/ML & DL/Face-Transformer/train.py\", line 468, in <module>\n",
      "    loss.backward()\n",
      "  File \"/home/debargha/venv/pipenv/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/debargha/venv/pipenv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 8 -w 0 -d retina -n VIT -head CosFace --outdir ./results/ViT-P8S8_retina_cosface_s1 --warmup-epochs 1 --lr 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P8S8_casia_cosface_s1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_ID [0]\n",
      "============================================================\n",
      "Overall Configurations:\n",
      "{'SEED': 1337, 'INPUT_SIZE': [112, 112], 'EMBEDDING_SIZE': 512, 'GPU_ID': [0], 'DEVICE': device(type='cuda', index=0), 'MULTI_GPU': False, 'NUM_EPOCH': 1, 'BATCH_SIZE': 46, 'DATA_ROOT': './Data/casia-webface/', 'EVAL_PATH': './eval/', 'BACKBONE_NAME': 'VIT', 'HEAD_NAME': 'CosFace', 'TARGET': ['lfw'], 'BACKBONE_RESUME_ROOT': '', 'WORK_PATH': './results/ViT-P8S8_casia_cosface_s1'}\n",
      "============================================================\n",
      "./Data/casia-webface/train.rec ./Data/casia-webface/train.idx\n",
      "header0 label [490624. 501196.]\n",
      "id2range 10572\n",
      "Number of Training Classes: 10572\n",
      "./eval/lfw.bin\n",
      "loading bin 0\n",
      "loading bin 1000\n",
      "loading bin 2000\n",
      "loading bin 3000\n",
      "loading bin 4000\n",
      "loading bin 5000\n",
      "loading bin 6000\n",
      "loading bin 7000\n",
      "loading bin 8000\n",
      "loading bin 9000\n",
      "loading bin 10000\n",
      "loading bin 11000\n",
      "torch.Size([12000, 3, 112, 112])\n",
      "ver lfw\n",
      "self.device_id [0]\n",
      "self.device_id [0]\n",
      "============================================================\n",
      "ViT_face(\n",
      "  (patch_to_embedding): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0-19): 20 x ModuleList(\n",
      "        (0): Residual(\n",
      "          (fn): PreNorm(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (1): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Residual(\n",
      "          (fn): PreNorm(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (1): GELU(approximate='none')\n",
      "                (2): Dropout(p=0.1, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (4): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_latent): Identity()\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (loss): CosFace(in_features = 512, out_features = 10572, s = 64.0, m = 0.35)\n",
      ")\n",
      "VIT Backbone Generated\n",
      "============================================================\n",
      "============================================================\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0003\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0003\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Optimizer Generated\n",
      "============================================================\n",
      "Epoch 1 Batch 10\tSpeed: 70.89 samples/s\tTraining Loss 35.0712 (35.5933)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 20\tSpeed: 77.14 samples/s\tTraining Loss 35.7815 (35.6009)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 30\tSpeed: 76.73 samples/s\tTraining Loss 36.0309 (35.3921)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 40\tSpeed: 76.52 samples/s\tTraining Loss 34.4152 (35.2535)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 50\tSpeed: 76.26 samples/s\tTraining Loss 35.1748 (35.2663)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 60\tSpeed: 76.22 samples/s\tTraining Loss 35.4578 (35.0291)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 70\tSpeed: 75.93 samples/s\tTraining Loss 35.4973 (35.1178)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 80\tSpeed: 75.72 samples/s\tTraining Loss 35.3257 (34.9738)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 90\tSpeed: 75.75 samples/s\tTraining Loss 34.7442 (35.0107)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 100\tSpeed: 75.64 samples/s\tTraining Loss 34.7190 (34.7325)\tTraining Prec@1 0.000 (0.000)\n",
      "Learning rate 0.000001\n",
      "Perform Evaluation on ['lfw'] , and Save Checkpoints...\n",
      "(12000, 512)\n",
      "[lfw][100]XNorm: 22.62780\n",
      "[lfw][100]Accuracy-Flip: 0.54650+-0.02066\n",
      "[lfw][100]Best-Threshold: 0.01000\n",
      "highest_acc: [0.5465]\n",
      "Epoch 1 Batch 110\tSpeed: 3.59 samples/s\tTraining Loss 34.6452 (34.8532)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 120\tSpeed: 74.85 samples/s\tTraining Loss 35.1385 (34.7375)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 130\tSpeed: 74.67 samples/s\tTraining Loss 34.3506 (34.7014)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 140\tSpeed: 74.74 samples/s\tTraining Loss 35.2404 (34.7131)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 150\tSpeed: 74.70 samples/s\tTraining Loss 34.2699 (34.6007)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 160\tSpeed: 74.53 samples/s\tTraining Loss 34.2465 (34.5590)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 170\tSpeed: 74.68 samples/s\tTraining Loss 35.0084 (34.7393)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 180\tSpeed: 74.61 samples/s\tTraining Loss 34.5477 (34.5346)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 190\tSpeed: 74.57 samples/s\tTraining Loss 35.1849 (34.6093)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 200\tSpeed: 74.49 samples/s\tTraining Loss 34.0744 (34.5729)\tTraining Prec@1 0.000 (0.000)\n",
      "Learning rate 0.000001\n",
      "Perform Evaluation on ['lfw'] , and Save Checkpoints...\n",
      "(12000, 512)\n",
      "[lfw][200]XNorm: 22.62835\n",
      "[lfw][200]Accuracy-Flip: 0.54250+-0.01982\n",
      "[lfw][200]Best-Threshold: 0.01000\n",
      "highest_acc: [0.5465]\n",
      "Epoch 1 Batch 210\tSpeed: 3.53 samples/s\tTraining Loss 35.0335 (34.4899)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 220\tSpeed: 74.37 samples/s\tTraining Loss 34.6926 (34.4762)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 230\tSpeed: 74.49 samples/s\tTraining Loss 34.7576 (34.3876)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 240\tSpeed: 74.26 samples/s\tTraining Loss 33.5773 (34.2820)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 250\tSpeed: 74.07 samples/s\tTraining Loss 34.7241 (34.3775)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 260\tSpeed: 74.23 samples/s\tTraining Loss 34.0226 (34.3018)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 270\tSpeed: 74.16 samples/s\tTraining Loss 34.1516 (34.3025)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 280\tSpeed: 74.15 samples/s\tTraining Loss 34.2939 (34.2193)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 290\tSpeed: 74.23 samples/s\tTraining Loss 34.5036 (34.3649)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 300\tSpeed: 74.03 samples/s\tTraining Loss 33.4511 (34.0604)\tTraining Prec@1 0.000 (0.000)\n",
      "Learning rate 0.000001\n",
      "Perform Evaluation on ['lfw'] , and Save Checkpoints...\n",
      "(12000, 512)\n",
      "[lfw][300]XNorm: 22.62883\n",
      "[lfw][300]Accuracy-Flip: 0.53650+-0.02165\n",
      "[lfw][300]Best-Threshold: 0.01000\n",
      "highest_acc: [0.5465]\n",
      "Epoch 1 Batch 310\tSpeed: 3.51 samples/s\tTraining Loss 34.7630 (34.4487)\tTraining Prec@1 0.000 (0.000)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debargha/Workspace/Projects/ML & DL/Face-Transformer/train.py\", line 452, in <module>\n",
      "    outputs, emb = BACKBONE(inputs.float(), labels)\n",
      "  File \"/home/debargha/venv/transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/debargha/Workspace/Projects/ML & DL/Face-Transformer/vit_pytorch/vit_face.py\", line 492, in forward\n",
      "    x = self.loss(emb, label)\n",
      "  File \"/home/debargha/venv/transformer/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/debargha/Workspace/Projects/ML & DL/Face-Transformer/vit_pytorch/vit_face.py\", line 197, in forward\n",
      "    one_hot = one_hot.cuda(self.device_id[0])\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 46 -w 0 -d casia -n VIT -head CosFace --outdir ./results/ViT-P8S8_casia_cosface_s1 --warmup-epochs 1 --lr 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P8S8_retina_cosface_s2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 2 -w 0 -d retina -n VIT -head CosFace --outdir ./results/ViT-P8S8_retina_cosface_s2 --warmup-epochs 0 --lr 1e-4 -r ./results/ViT-P8S8_retina_cosface_s1/Backbone_VIT_LR_checkpoint.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P8S8_casia_cosface_s2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_ID [0]\n",
      "============================================================\n",
      "Overall Configurations:\n",
      "{'SEED': 1337, 'INPUT_SIZE': [112, 112], 'EMBEDDING_SIZE': 512, 'GPU_ID': [0], 'DEVICE': device(type='cuda', index=0), 'MULTI_GPU': False, 'NUM_EPOCH': 1, 'BATCH_SIZE': 16, 'DATA_ROOT': './Data/casia-webface/', 'EVAL_PATH': './eval/', 'BACKBONE_NAME': 'VIT', 'HEAD_NAME': 'CosFace', 'TARGET': ['lfw'], 'BACKBONE_RESUME_ROOT': './results/ViT-P8S8_casia_cosface_s1/Backbone_VIT_LR_checkpoint.pth', 'WORK_PATH': './results/ViT-P8S8_casia_cosface_s2'}\n",
      "============================================================\n",
      "./Data/casia-webface/train.rec ./Data/casia-webface/train.idx\n",
      "header0 label [490624. 501196.]\n",
      "id2range 10572\n",
      "Number of Training Classes: 10572\n",
      "./eval/lfw.bin\n",
      "loading bin 0\n",
      "loading bin 1000\n",
      "loading bin 2000\n",
      "loading bin 3000\n",
      "loading bin 4000\n",
      "loading bin 5000\n",
      "loading bin 6000\n",
      "loading bin 7000\n",
      "loading bin 8000\n",
      "loading bin 9000\n",
      "loading bin 10000\n",
      "loading bin 11000\n",
      "torch.Size([12000, 3, 112, 112])\n",
      "ver lfw\n",
      "self.device_id [0]\n",
      "self.device_id [0]\n",
      "============================================================\n",
      "ViT_face(\n",
      "  (patch_to_embedding): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0-19): 20 x ModuleList(\n",
      "        (0): Residual(\n",
      "          (fn): PreNorm(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (1): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Residual(\n",
      "          (fn): PreNorm(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (1): GELU(approximate='none')\n",
      "                (2): Dropout(p=0.1, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (4): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_latent): Identity()\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (loss): CosFace(in_features = 512, out_features = 10572, s = 64.0, m = 0.35)\n",
      ")\n",
      "VIT Backbone Generated\n",
      "============================================================\n",
      "============================================================\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.05\n",
      ")\n",
      "Optimizer Generated\n",
      "============================================================\n",
      "============================================================\n",
      "./results/ViT-P8S8_casia_cosface_s1/Backbone_VIT_LR_checkpoint.pth\n",
      "Loading Backbone Checkpoint './results/ViT-P8S8_casia_cosface_s1/Backbone_VIT_LR_checkpoint.pth'\n",
      "============================================================\n",
      "Epoch 1 Batch 310\tSpeed: 18.92 samples/s\tTraining Loss 32.8663 (33.8678)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 320\tSpeed: 60.92 samples/s\tTraining Loss 32.9453 (33.5454)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 330\tSpeed: 60.77 samples/s\tTraining Loss 33.0793 (33.3312)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 340\tSpeed: 60.74 samples/s\tTraining Loss 32.7085 (33.0181)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 350\tSpeed: 60.64 samples/s\tTraining Loss 33.2518 (32.7192)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 360\tSpeed: 60.54 samples/s\tTraining Loss 33.1358 (32.7552)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 370\tSpeed: 60.51 samples/s\tTraining Loss 32.7001 (32.9594)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 380\tSpeed: 60.49 samples/s\tTraining Loss 33.3830 (32.6004)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 390\tSpeed: 60.40 samples/s\tTraining Loss 32.8106 (32.3370)\tTraining Prec@1 0.000 (0.000)\n",
      "Epoch 1 Batch 400\tSpeed: 60.25 samples/s\tTraining Loss 32.6350 (32.6101)\tTraining Prec@1 0.000 (0.000)\n",
      "Learning rate 0.000100\n",
      "Perform Evaluation on ['lfw'] , and Save Checkpoints...\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debargha/Workspace/Projects/ML & DL/Face-Transformer/train.py\", line 511, in <module>\n",
      "    accuracy, std, xnorm, best_threshold, roc_curve = perform_val(\n",
      "  File \"/home/debargha/Workspace/Projects/ML & DL/Face-Transformer/util/utils.py\", line 179, in perform_val\n",
      "    temp = backbone(batch.to(device)).cpu()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 16 -w 0 -d casia -n VIT -head CosFace --outdir ./results/ViT-P8S8_casia_cosface_s2 --warmup-epochs 0 --lr 1e-4 -r ./results/ViT-P8S8_casia_cosface_s1/Backbone_VIT_LR_checkpoint.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P8S8_retina_cosface_s3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 13 -w 0 -d retina -n VIT -head CosFace --outdir ./results/ViT-P8S8_retina_cosface_s3 --warmup-epochs 0 --lr 5e-5 -r ./results/ViT-P8S8_retina_cosface_s2/Backbone_VIT_LR_checkpoint.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P8S8_casia_cosface_s3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 16 -w 0 -d casia -n VIT -head CosFace --outdir ./results/ViT-P8S8_casia_cosface_s3 --warmup-epochs 0 --lr 5e-5 -r ./results/ViT-P8S8_casia_cosface_s2/Backbone_VIT_LR_checkpoint.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ViT-P12S8`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P12S8_retina_cosface_s1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 13 -w 0 -d retina -n VITs -head CosFace --outdir ./results/ViTs-P12S8_retina_cosface_s1 --warmup-epochs 1 --lr 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P12S8_casia_cosface_s1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 16 -w 0 -d casia -n VITs -head CosFace --outdir ./results/ViTs-P12S8_casia_cosface_s1 --warmup-epochs 1 --lr 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P12S8_retina_cosface_s2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 13 -w 0 -d retina -n VITs -head CosFace --outdir ./results/ViTs-P12S8_retina_cosface_s2 --warmup-epochs 0 --lr 1e-4 -r ./results/ViTs-P12S8_casia_cosface_s1/Backbone_VITs_LR_checkpoint.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P12S8_casia_cosface_s2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 16 -w 0 -d casia -n VITs -head CosFace --outdir ./models/ViTs-P12S8_casia_cosface_s2 --warmup-epochs 0 --lr 1e-4 -r ./results/ViTs-P12S8_casia_cosface_s1/Backbone_VITs_LR_checkpoint.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P12S8_retina_cosface_s3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 13 -w 0 -d retina -n VITs -head CosFace --outdir ./results/ViTs-P12S8_retina_cosface_s3 --warmup-epochs 0 --lr 5e-5 -r ./results/ViTs-P12S8_retina_cosface_s2/Backbone_VITs_LR_checkpoint.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### `ViT-P12S8_casia_cosface_s3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES='0' python3 -u train.py -b 16 -w 0 -d casia -n VITs -head CosFace --outdir ./models/ViT-P12S8_casia_cosface_s3 --warmup-epochs 0 --lr 5e-5 -r ./results/ViTs-P12S8_casia_cosface_s2/Backbone_VITs_LR_checkpoint.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
